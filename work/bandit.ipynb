{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import imp\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import pyrl.policygradient as PG\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/hongli/Documents/pyrlearning/examples')\n",
    "sys.path.append('/home/hongli/Documents/pyrlearning/pyrl')\n",
    "sys.path.append('/home/hongli/Documents/pyrlearning/examples/models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = 'models/twoarmbandit'\n",
    "action = 'train'\n",
    "modelfile = os.path.abspath(model_file)\n",
    "if not modelfile.endswith('.py'):\n",
    "    modelfile += '.py'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODELFILE: /home/hongli/Documents/RLearning/models/twoarmbandit.py\nACTION:    train\nARGS:      []\nSEED:      100\nSUFFIX:    \nGPU:       False\n"
     ]
    }
   ],
   "source": [
    "# default settings\n",
    "args = []\n",
    "dt = 100\n",
    "dt_save = 0\n",
    "seed = 100\n",
    "suffix = ''\n",
    "gpu = False\n",
    "print(\"MODELFILE: \" + modelfile)\n",
    "print(\"ACTION:    \" + action)\n",
    "print(\"ARGS:      \" + str(args))\n",
    "print(\"SEED:      \" + str(seed))\n",
    "print(\"SUFFIX:    \" + suffix)\n",
    "print(\"GPU:       \" + str(gpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set flags before importing Theano\n",
    "os.environ.setdefault('THEANO_FLAGS', '')\n",
    "os.environ['THEANO_FLAGS'] += ',floatX=float32,allow_gc=False'\n",
    "if gpu:\n",
    "    os.environ['THEANO_FLAGS'] += ',device=gpu,nvcc.fastmath=True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================================================================================\n",
    "# Setup paths\n",
    "#=========================================================================================\n",
    "\n",
    "from pyrl import utils\n",
    "from pyrl.model import Model\n",
    "\n",
    "# Location of script\n",
    "# here   = utils.get_here(sys.argv[0])\n",
    "# __file__ won't work in interactive interpretor \n",
    "here = utils.get_here('do.py') \n",
    "prefix = os.path.basename(here)\n",
    "\n",
    "# Name to use\n",
    "name = os.path.splitext(os.path.basename(modelfile))[0] + suffix\n",
    "\n",
    "# Scratch\n",
    "scratchpath = os.environ.get('SCRATCH')\n",
    "if scratchpath is None:\n",
    "    scratchpath = os.path.join(os.environ['HOME'], 'scratch')\n",
    "trialspath = os.path.join(scratchpath, 'work', 'pyrl', prefix, name)\n",
    "\n",
    "# Paths\n",
    "workpath = os.path.join(here,     'work')\n",
    "datapath = os.path.join(workpath, 'data', name)\n",
    "figspath = os.path.join(workpath, 'figs', name)\n",
    "\n",
    "# Create necessary directories\n",
    "for path in [datapath, figspath, trialspath]:\n",
    "    utils.mkdir_p(path)\n",
    "\n",
    "# File to store model in\n",
    "savefile = os.path.join(datapath, name + '.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Task', pyrl.model.Task),\n ('__class__', pyrl.model.Model),\n ('__delattr__',\n  <method-wrapper '__delattr__' of Model object at 0x7fd6353ef610>),\n ('__dict__',\n  {'Task': pyrl.model.Task,\n   'config': {'L1_Wrec': 0,\n    'L2_Wrec': 0,\n    'L2_r': 0,\n    'N': 100,\n    'Nin': 2,\n    'Nout': 2,\n    'Performance': pyrl.performance.Performance2AFC,\n    'R_ABORTED': 0,\n    'R_TERMINAL': None,\n    'Win': 1,\n    'Win_mask': None,\n    'abort_on_last_t': True,\n    'actions': OrderedDict([('CHOOSE-LEFT', 0), ('CHOOSE-RIGHT', 1)]),\n    'baseline_L2_r': 0,\n    'baseline_N': 100,\n    'baseline_Win': 10,\n    'baseline_Win_mask': None,\n    'baseline_bout': None,\n    'baseline_fix': [],\n    'baseline_lr': 0.004,\n    'baseline_network_type': 'gru',\n    'baseline_p0': 1,\n    'baseline_rho': 2,\n    'baseline_seed': 2,\n    'baseline_var_rec': 0.01,\n    'bout': 0.5,\n    'checkfreq': 50,\n    'dt': 10,\n    'fix': [],\n    'inputs': OrderedDict([('Choice', 0), ('Reward', 1)]),\n    'lr': 0.004,\n    'max_iter': 100000,\n    'mode': 'continuous',\n    'n_gradient': 2,\n    'n_validation': 100,\n    'network_type': 'gru',\n    'p0': 0.1,\n    'policy_seed': 1,\n    'rho': 2,\n    'target_reward': inf,\n    'tau': 100,\n    'tau_reward': inf,\n    'tmax': 8200,\n    'var_rec': 0.01},\n   'spec': <module 'model' from '/home/hongli/Documents/RLearning/models/twoarmbandit.pyc'>,\n   'task': <pyrl.model.Task at 0x7fd6353ef910>}),\n ('__doc__', None),\n ('__format__', <function __format__>),\n ('__getattribute__',\n  <method-wrapper '__getattribute__' of Model object at 0x7fd6353ef610>),\n ('__hash__', <method-wrapper '__hash__' of Model object at 0x7fd6353ef610>),\n ('__init__',\n  <bound method Model.__init__ of <pyrl.model.Model object at 0x7fd6353ef610>>),\n ('__module__', 'pyrl.model'),\n ('__new__', <function __new__>),\n ('__reduce__', <function __reduce__>),\n ('__reduce_ex__', <function __reduce_ex__>),\n ('__repr__', <method-wrapper '__repr__' of Model object at 0x7fd6353ef610>),\n ('__setattr__',\n  <method-wrapper '__setattr__' of Model object at 0x7fd6353ef610>),\n ('__sizeof__', <function __sizeof__>),\n ('__str__', <method-wrapper '__str__' of Model object at 0x7fd6353ef610>),\n ('__subclasshook__', <function __subclasshook__>),\n ('__weakref__', None),\n ('config',\n  {'L1_Wrec': 0,\n   'L2_Wrec': 0,\n   'L2_r': 0,\n   'N': 100,\n   'Nin': 2,\n   'Nout': 2,\n   'Performance': pyrl.performance.Performance2AFC,\n   'R_ABORTED': 0,\n   'R_TERMINAL': None,\n   'Win': 1,\n   'Win_mask': None,\n   'abort_on_last_t': True,\n   'actions': OrderedDict([('CHOOSE-LEFT', 0), ('CHOOSE-RIGHT', 1)]),\n   'baseline_L2_r': 0,\n   'baseline_N': 100,\n   'baseline_Win': 10,\n   'baseline_Win_mask': None,\n   'baseline_bout': None,\n   'baseline_fix': [],\n   'baseline_lr': 0.004,\n   'baseline_network_type': 'gru',\n   'baseline_p0': 1,\n   'baseline_rho': 2,\n   'baseline_seed': 2,\n   'baseline_var_rec': 0.01,\n   'bout': 0.5,\n   'checkfreq': 50,\n   'dt': 10,\n   'fix': [],\n   'inputs': OrderedDict([('Choice', 0), ('Reward', 1)]),\n   'lr': 0.004,\n   'max_iter': 100000,\n   'mode': 'continuous',\n   'n_gradient': 2,\n   'n_validation': 100,\n   'network_type': 'gru',\n   'p0': 0.1,\n   'policy_seed': 1,\n   'rho': 2,\n   'target_reward': inf,\n   'tau': 100,\n   'tau_reward': inf,\n   'tmax': 8200,\n   'var_rec': 0.01}),\n ('get_pg',\n  <bound method Model.get_pg of <pyrl.model.Model object at 0x7fd6353ef610>>),\n ('spec',\n  <module 'model' from '/home/hongli/Documents/RLearning/models/twoarmbandit.pyc'>),\n ('task', <pyrl.model.Task at 0x7fd6353ef910>),\n ('train',\n  <bound method Model.train of <pyrl.model.Model object at 0x7fd6353ef610>>)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "inspect.getmembers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.train(savefile, seed, recover=('recover' in args))\n",
    "config = model.config\n",
    "recover = False\n",
    "Task = model.Task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hongli/Documents/RLearning/work/data/twoarmbandit/twoarmbandit.pkl\n100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('__class__', type),\n ('__delattr__', <slot wrapper '__delattr__' of 'object' objects>),\n ('__dict__',\n  dict_proxy({'__dict__': <attribute '__dict__' of 'Task' objects>,\n              '__doc__': None,\n              '__init__': <function pyrl.model.__init__>,\n              '__module__': 'pyrl.model',\n              '__weakref__': <attribute '__weakref__' of 'Task' objects>})),\n ('__doc__', None),\n ('__format__', <method '__format__' of 'object' objects>),\n ('__getattribute__', <slot wrapper '__getattribute__' of 'object' objects>),\n ('__hash__', <slot wrapper '__hash__' of 'object' objects>),\n ('__init__', <unbound method Task.__init__>),\n ('__module__', 'pyrl.model'),\n ('__new__', <function __new__>),\n ('__reduce__', <method '__reduce__' of 'object' objects>),\n ('__reduce_ex__', <method '__reduce_ex__' of 'object' objects>),\n ('__repr__', <slot wrapper '__repr__' of 'object' objects>),\n ('__setattr__', <slot wrapper '__setattr__' of 'object' objects>),\n ('__sizeof__', <method '__sizeof__' of 'object' objects>),\n ('__str__', <slot wrapper '__str__' of 'object' objects>),\n ('__subclasshook__', <function __subclasshook__>),\n ('__weakref__', <attribute '__weakref__' of 'Task' objects>)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print savefile\n",
    "print seed\n",
    "inspect.getmembers(Task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PolicyGradient' object has no attribute 'PolicyGradient'",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-19d0ec298c91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# pg = get_pg(config, config['seed'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mpg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPolicyGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'best'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'PolicyGradient' object has no attribute 'PolicyGradient'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# this part is the break down of inside of model.train...\n",
    "if recover and os.path.isfile(savefile):\n",
    "    pg = self.get_pg(savefile, load='current')  # get_pg returns a policygradient instance\n",
    "else:\n",
    "    config['seed']          = 3*seed\n",
    "    config['policy_seed']   = 3*seed + 1\n",
    "    config['baseline_seed'] = 3*seed + 2\n",
    "\n",
    "    # pg = get_pg(config, config['seed'])\n",
    "    pg = PG.PolicyGradient(Task, config, seed=seed, dt=config['dt'], load='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<< RNG 100 >> __main__\n"
     ]
    }
   ],
   "source": [
    "# get policy gradient\n",
    "# try to break down PolicyGradient setup\n",
    "# this block is equivalent to \"pg.PolicyGradient(task, config, seed=seed, dt=config['dt'], load='best')\"\n",
    "from pyrl.networks import Networks\n",
    "\n",
    "task = Task()\n",
    "\n",
    "# =================================================================================\n",
    "# Network setup\n",
    "# =================================================================================\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Create new model.\n",
    "# #-----------------------------------------------------------------------------\n",
    "\n",
    "# config comes from configs.py\n",
    "# print \"config is\", config\n",
    "\n",
    "# dt = 10ms\n",
    "# Leak\n",
    "alpha = dt/config['tau']\n",
    "\n",
    "# Policy network\n",
    "K = config['p0']*config['N']\n",
    "policy_config = {\n",
    "    'Nin':      config['Nin'],\n",
    "    'N':        config['N'],\n",
    "    'Nout':     config['Nout'],\n",
    "    'p0':       config['p0'],\n",
    "    'rho':      config['rho'],\n",
    "    'f_out':    'softmax',\n",
    "    'Win':      config['Win']*np.sqrt(K)/config['Nin'],\n",
    "    'Win_mask': config['Win_mask'],\n",
    "    'fix':      config['fix'],\n",
    "    'L2_r':     config['L2_r'],\n",
    "    'L1_Wrec':  config['L1_Wrec'],\n",
    "    'L2_Wrec':  config['L2_Wrec'],\n",
    "    'alpha':    alpha,\n",
    "    'bout':     config['bout']\n",
    "}\n",
    "\n",
    "# print \"bout:\", self.policy_config['bout']\n",
    "\n",
    "# Network type\n",
    "Network = Networks[config['network_type']]\n",
    "policy_net = Network(policy_config,\n",
    "    seed=config['policy_seed'], name='policy')\n",
    "\n",
    "# Baseline network\n",
    "# Win = np.zeros((self.policy_net.N + len(config['actions']), 3*config['N']))\n",
    "# Win[self.policy_net.N:] = 1\n",
    "# what's baseline?\n",
    "\n",
    "'''\n",
    "            rng = np.random.RandomState(1234)\n",
    "            policy_N     = config['N']\n",
    "            baseline_N   = config['N']\n",
    "            baseline_Nin = self.policy_net.N + len(config['actions'])\n",
    "            baseline_Win_mask = np.zeros((baseline_Nin, 3*baseline_N))\n",
    "            p_in = 0.5\n",
    "            baseline_Win_mask[:policy_N] = (rng.uniform(size=baseline_Win_mask[:policy_N].shape) < p_in)\n",
    "            #baseline_Win = 1/np.sqrt(p_in*baseline)\n",
    "'''\n",
    "\n",
    "K = config['baseline_p0']*config['N']\n",
    "baseline_Nin = policy_net.N + len(config['actions'])\n",
    "baseline_config = {\n",
    "    'Nin':      baseline_Nin,\n",
    "    'N':        config['baseline_N'],\n",
    "    'Nout':     1,\n",
    "    'p0':       config['baseline_p0'],\n",
    "    'rho':      config['baseline_rho'],\n",
    "    'f_out':    'linear',\n",
    "    'Win':      config['baseline_Win']*np.sqrt(K)/baseline_Nin,\n",
    "    'Win_mask': config['baseline_Win_mask'],\n",
    "    'bout':     config['baseline_bout'],\n",
    "    'fix':      config['baseline_fix'],\n",
    "    'L2_r':     config['baseline_L2_r'],\n",
    "    'L1_Wrec':  config['L1_Wrec'],\n",
    "    'L2_Wrec':  config['L2_Wrec'],\n",
    "    'alpha':    alpha\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<< RNG 100 >> __main__\n"
     ]
    }
   ],
   "source": [
    "# PolicyGradient part 2\n",
    "\n",
    "if baseline_config['bout'] is None:\n",
    "    baseline_config['bout'] = config['R_ABORTED']\n",
    "\n",
    "# Network type\n",
    "Network = Networks[config.get('baseline_network_type',\n",
    "                                    config['network_type'])]\n",
    "baseline_net = Network(baseline_config,\n",
    "                                    seed=config['baseline_seed'], name='baseline')\n",
    "\n",
    "#=================================================================================\n",
    "# PG setup\n",
    "#=================================================================================\n",
    "\n",
    "# Network structure\n",
    "Nin  = config['Nin']\n",
    "N    = config['N']\n",
    "Nout = config['Nout']\n",
    "\n",
    "# Number of actions\n",
    "n_actions = len(config['actions'])\n",
    "\n",
    "# Recurrent noise, scaled by `2*tau/dt`\n",
    "scaled_var_rec = (2*config['tau']/dt) * config['var_rec']\n",
    "scaled_baseline_var_rec = ((2*config['tau']/dt)\n",
    "                                * config['baseline_var_rec'])\n",
    "\n",
    "# Run trials continuously?\n",
    "mode = config['mode']\n",
    "if mode == 'continuous':\n",
    "    step_0_states = policy_net.func_step_0(True)\n",
    "\n",
    "# Maximum length of a trial\n",
    "Tmax = int(config['tmax']/config['dt']) + 1\n",
    "\n",
    "# Discount future reward\n",
    "if np.isfinite(config['tau_reward']):\n",
    "    alpha_reward = dt/config['tau_reward']\n",
    "    def discount_factor(t):\n",
    "        return np.exp(-t*alpha_reward)\n",
    "else:\n",
    "    def discount_factor(t):\n",
    "        return 1\n",
    "discount_factor = discount_factor\n",
    "\n",
    "# Reward on aborted trials\n",
    "abort_on_last_t = config.get('abort_on_last_t', True)\n",
    "if 'R_TERMINAL' in config and config['R_TERMINAL'] is not None:\n",
    "    R_TERMINAL = config['R_TERMINAL']\n",
    "else:\n",
    "    R_TERMINAL = config['R_ABORTED']\n",
    "    R_ABORTED = config['R_ABORTED']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<< RNG 100 >> __main__\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Random number generator\n",
    "import pyrl.nptools as nptools\n",
    "rng = nptools.get_rng(seed, __name__)\n",
    "\n",
    "# Compile functions\n",
    "policy_step_0   = policy_net.func_step_0()\n",
    "policy_step_t   = policy_net.func_step_t()\n",
    "baseline_step_0 = baseline_net.func_step_0()\n",
    "baseline_step_t = baseline_net.func_step_t()\n",
    "\n",
    "# Performance\n",
    "Performance = config['Performance']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# break down pg.train here\n",
    "\"\"\"\n",
    "Train network.\n",
    "\n",
    "\"\"\"\n",
    "import pyrl.theanotools as theanotools\n",
    "from   collections import OrderedDict\n",
    "from   theano import tensor\n",
    "from pyrl.sgd import Adam\n",
    "#=================================================================================\n",
    "# Parameters\n",
    "#=================================================================================\n",
    "\n",
    "max_iter     = config['max_iter']\n",
    "lr           = config['lr']\n",
    "baseline_lr  = config['baseline_lr']\n",
    "n_gradient   = config['n_gradient']\n",
    "n_validation = config['n_validation']\n",
    "checkfreq    = config['checkfreq']\n",
    "\n",
    "if mode == 'continuous':\n",
    "    print(\"[ PolicyGradient.train ] Continuous mode.\")\n",
    "    use_x0 = True\n",
    "else:\n",
    "    use_x0 = False\n",
    "\n",
    "# GPU?\n",
    "if theanotools.get_processor_type() == 'gpu':\n",
    "    gpu = 'yes'\n",
    "else:\n",
    "    gpu = 'no'\n",
    "\n",
    "# Print settings\n",
    "items = OrderedDict()\n",
    "items['GPU']                      = gpu\n",
    "items['Network type (policy)']    = config['network_type']\n",
    "items['Network type (baseline)']  = config.get('baseline_network_type',\n",
    "                                                            config['network_type'])\n",
    "items['N (policy)']               = config['N']\n",
    "items['N (baseline)']             = config['baseline_N']\n",
    "items['Conn. prob. (policy)']     = config['p0']\n",
    "items['Conn. prob. (baseline)']   = config['baseline_p0']\n",
    "items['dt']                       = str(dt) + ' ms'\n",
    "items['tau_reward']               = str(config['tau_reward']) + ' ms'\n",
    "items['var_rec (policy)']         = config['var_rec']\n",
    "items['var_rec (baseline)']       = config['baseline_var_rec']\n",
    "items['Learning rate (policy)']   = config['lr']\n",
    "items['Learning rate (baseline)'] = config['baseline_lr']\n",
    "items['Max time steps']           = Tmax\n",
    "items['Num. trials (gradient)']   = config['n_gradient']\n",
    "items['Num. trials (validation)'] = config['n_validation']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def func_update_policy(Tmax, use_x0=False, accumulators=None):\n",
    "    \"\"\"\n",
    "    update policy network\n",
    "    :param Tmax:\n",
    "    :param use_x0:\n",
    "    :param accumulators:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    U = tensor.tensor3('U') # Inputs\n",
    "    Q = tensor.tensor3('Q') # Noise\n",
    "\n",
    "    if use_x0:\n",
    "        x0_ = tensor.matrix('x0_')\n",
    "    else:\n",
    "        x0  = policy_net.params['x0']\n",
    "        x0_ = tensor.alloc(x0, U.shape[1], x0.shape[0])\n",
    "\n",
    "    log_z_0  = policy_net.get_outputs_0(x0_, log=True)\n",
    "    r, log_z = policy_net.get_outputs(U, Q, x0_, log=True)\n",
    "\n",
    "    # Learning rate\n",
    "    lr = tensor.scalar('lr')\n",
    "\n",
    "    A = tensor.tensor3('A')\n",
    "    R = tensor.matrix('R')\n",
    "    b = tensor.matrix('b')\n",
    "    M = tensor.matrix('M')\n",
    "\n",
    "    logpi_0 = tensor.sum(log_z_0*A[0], axis=-1)*M[0]\n",
    "    logpi_t = tensor.sum(log_z*A[1:],  axis=-1)*M[1:]\n",
    "\n",
    "    # Entropy\n",
    "    # entropy_0 = tensor.sum(tensor.exp(log_z_0)*log_z_0, axis=-1)*M[0]\n",
    "    # entropy_t = tensor.sum(tensor.exp(log_z)*log_z, axis=-1)*M[1:]\n",
    "    # entropy   = (tensor.sum(entropy_0) + tensor.sum(entropy_t))/tensor.sum(M)\n",
    "\n",
    "    # def f(x):\n",
    "    #    return -x**2/2/sigma**2\n",
    "\n",
    "    # logpi_0 = tensor.sum(f(A[0] - z_0), axis=-1)*M[0]\n",
    "    # logpi_t = tensor.sum(f(A[1:] - z), axis=-1)*M[1:]\n",
    "\n",
    "    # Enforce causality\n",
    "    Mcausal = theanotools.zeros((Tmax-1, Tmax-1))\n",
    "    for i in xrange(Mcausal.shape[0]):\n",
    "        Mcausal[i,i:] = 1\n",
    "    Mcausal = theanotools.shared(Mcausal, 'Mcausal')\n",
    "\n",
    "    J0 = logpi_0*R[0]\n",
    "    J0 = tensor.mean(J0)\n",
    "    J  = (logpi_t.T).dot(Mcausal).dot(R[1:]*M[1:])\n",
    "    J  = tensor.nlinalg.trace(J)/J.shape[0]\n",
    "\n",
    "    J += J0\n",
    "\n",
    "    # Second term\n",
    "    Jb0 = logpi_0*b[0]\n",
    "    Jb0 = tensor.mean(Jb0)\n",
    "    Jb  = logpi_t*b[1:]\n",
    "    Jb  = tensor.mean(tensor.sum(Jb, axis=0))\n",
    "\n",
    "    J -= Jb0 + Jb\n",
    "\n",
    "    # Objective function\n",
    "    obj = -J + policy_net.get_regs(x0_, r, M)# + 0.0005*entropy\n",
    "\n",
    "    # SGD\n",
    "    policy_sgd = Adam(policy_net.trainables, accumulators=accumulators)\n",
    "    if policy_net.type == 'simple':\n",
    "        i = policy_net.index('Wrec')\n",
    "        grads = tensor.grad(obj, policy_net.trainables)\n",
    "        grads[i] += policy_net.get_dOmega_dWrec(-J, r)\n",
    "        norm, grads, updates = policy_sgd.get_updates(obj, lr, grads=grads)\n",
    "    else:\n",
    "        norm, grads, updates = policy_sgd.get_updates(obj, lr)\n",
    "\n",
    "    if use_x0:\n",
    "        args = [x0_]\n",
    "    else:\n",
    "        args = []\n",
    "    args += [U, Q, A, R, b, M, lr]\n",
    "\n",
    "    return theano.function(args, norm, updates=updates)\n",
    "\n",
    "def func_update_baseline(self, use_x0=False, accumulators=None):\n",
    "    \"\"\"\n",
    "    update value network\n",
    "    the reward baseline is the output of a recurrently connected value work\n",
    "    useful for learning but not executing\n",
    "    expected reward\n",
    "    :param use_x0:\n",
    "    :param accumulators:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    U  = tensor.tensor3('U')\n",
    "    R  = tensor.matrix('R')\n",
    "    R_ = R.reshape((R.shape[0], R.shape[1], 1))\n",
    "    Q  = tensor.tensor3('Q')\n",
    "\n",
    "    if use_x0:\n",
    "        x0_ = tensor.matrix('x0_')\n",
    "    else:\n",
    "        x0  = baseline_net.params['x0']\n",
    "        x0_ = tensor.alloc(x0, U.shape[1], x0.shape[0])\n",
    "\n",
    "    z_0   = baseline_net.get_outputs_0(x0_)\n",
    "    r, z  = baseline_net.get_outputs(U, Q, x0_)\n",
    "    z_all = tensor.concatenate([z_0.reshape((1, z_0.shape[0], z_0.shape[1])), z],\n",
    "                                   axis=0)\n",
    "\n",
    "    # Learning rate\n",
    "    lr = tensor.scalar('lr')\n",
    "\n",
    "    # Reward prediction error\n",
    "    M    = tensor.matrix('M')\n",
    "    L2   = tensor.sum((tensor.sqr(z_all[:,:,0] - R))*M)/tensor.sum(M)\n",
    "    RMSE = tensor.sqrt(L2)\n",
    "\n",
    "    # Objective function\n",
    "    obj = L2 + baseline_net.get_regs(x0_, r, M)\n",
    "\n",
    "    # SGD\n",
    "    baseline_sgd = Adam(baseline_net.trainables, accumulators=accumulators)\n",
    "    if baseline_net.type == 'simple':\n",
    "        i = baseline_net.index('Wrec')\n",
    "        grads = tensor.grad(obj, baseline_net.trainables)\n",
    "        grads[i] += baseline_net.get_dOmega_dWrec(L2, r)\n",
    "        norm, grads, updates = baseline_sgd.get_updates(obj, lr, grads=grads)\n",
    "    else:\n",
    "        norm, grads, updates = baseline_sgd.get_updates(obj, lr)\n",
    "\n",
    "    if use_x0:\n",
    "        args = [x0_]\n",
    "\n",
    "    else:\n",
    "        args = []\n",
    "    args += [U, Q, R, M, lr]\n",
    "\n",
    "    return theano.function(args, [z_all[:,:,0], norm, RMSE], updates=updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elemwise{add,no_inplace}.0\n[Win, bin, Wrec_gates, Wrec, Wout, bout, x0]\n[[ 5.89712731e-01  2.75748673e+00  5.61331714e-01  4.81714857e-01\n   3.51221732e+00 -5.25251158e-01 -2.65258864e+00 -2.42043371e+00\n   1.94426919e+00  1.30909400e+00  6.68499229e-01 -1.85455627e+00\n  -2.02361209e+00  3.90004370e+00  4.42836822e-01 -4.74619394e-01\n  -3.07157585e+00 -2.55827576e+00 -1.53741054e+00 -8.90633411e-02\n  -1.40086121e+00 -1.24831680e+00 -2.54821933e+00 -9.73235731e-01\n   1.13957379e+00  6.14914435e-01 -3.61713510e-01 -1.13549250e+00\n   2.11210670e+00  6.58979620e-01 -2.72836021e-01  1.45464528e+00\n  -2.00884598e+00 -1.17365406e+00  1.65145322e+00 -1.74988004e+00\n   2.06998873e+00  5.71356138e-01 -2.63470722e+00 -1.01879056e+00\n  -1.27141802e+00  4.19846474e-01 -1.00506500e+00  1.18373485e+00\n   6.15645424e-01  1.02742268e+00  7.95002938e-01 -4.30618073e+00\n   2.49418567e+00  2.30871982e-01  9.92137529e-01 -3.89853154e-01\n  -2.30071069e+00 -6.60273529e-01 -1.01405497e+00 -7.01146499e-01\n  -5.12740397e-01  6.26174479e-01  2.37571240e-01 -5.59884204e-01\n  -1.92618933e+00 -3.85365401e-02 -1.74245841e+00 -4.42081696e-01\n   1.15200037e+00  1.24422108e+00 -5.68225240e-01 -1.37552512e+00\n   4.45829798e+00 -2.76496672e+00 -2.37010793e-01 -1.75088806e+00\n   2.31018733e+00  2.38397140e+00 -1.81440962e+00  4.13411892e-01\n   2.35219807e+00 -1.14137171e+00  1.46442854e+00  4.05089452e-01\n  -1.89019025e+00 -4.36686834e-02 -1.35660839e+00  6.23894030e-01\n   1.30549305e+00 -1.56701337e+00 -1.92951318e+00 -1.45217912e+00\n   1.75154493e+00 -9.32821047e-01  3.32011776e-01  1.84780553e+00\n  -2.82861079e-01  7.57895162e-01 -1.54792086e+00  2.83268833e+00\n  -7.66919382e-01 -1.81917480e+00  9.64167497e-01  1.90623171e-01\n   4.92263715e-01  1.59211420e+00  1.44017254e+00 -2.48642309e-01\n  -3.37766461e-01 -2.27551753e+00 -1.29208083e+00 -3.67942958e-01\n   6.97932877e-01 -1.46762770e+00  6.63114636e-01 -1.61846335e+00\n   4.49034227e+00  4.44484692e-01  9.21491520e-01 -1.75745954e-01\n   1.16323925e+00  3.43786828e+00 -1.63315141e+00 -1.18851205e+00\n   2.06633129e+00  3.97735474e-01 -2.68139461e-02  1.47883790e+00\n   1.24568539e+00  2.27969910e+00  8.47134132e-01  1.59252372e-01\n  -1.43922267e+00  6.21847492e-01  6.35212977e-01 -1.80903273e+00\n  -8.60896585e-02 -3.83253305e-01  2.83435604e+00 -1.81688910e+00\n  -3.45212414e+00  1.30039493e+00  3.24272265e-01  3.02987715e+00\n  -2.21481253e-01 -1.59706642e+00  4.83646753e-01 -5.28589681e-01\n  -7.39026342e-01  2.41534451e+00 -1.64732025e+00  5.67988470e-01\n  -2.09596162e+00  2.06721224e-02 -7.78035694e-01  1.04057813e+00\n  -3.61538826e-02  1.69929675e+00 -2.47777337e-01 -2.58469053e+00\n  -9.68215536e-01  2.33858238e-01  7.21729788e-01  1.71350408e+00\n   4.91717651e-02  1.35440228e+00  1.87343074e+00 -1.44870543e+00\n   2.72153354e-01 -1.92470949e+00 -3.16898313e+00  5.09394982e-01\n  -1.91410677e+00 -2.12787325e+00 -1.17244951e-01 -1.48932015e+00\n  -1.98451554e+00  2.85181423e+00 -1.36814663e+00 -2.69907550e+00\n  -5.23061375e-01 -5.01566230e-01 -8.23489333e-02 -1.06644575e+00\n  -1.20586279e+00 -1.22560119e+00  1.74595704e+00  2.64474734e-02\n   4.51285172e-01  9.80667179e-01  6.05524995e-01  6.99530443e-01\n  -1.68315602e+00 -1.53200776e+00  1.43510468e-01 -1.50976887e+00\n  -1.09455409e+00  1.34970634e+00 -2.86261998e+00 -7.66194415e-01\n   1.48241719e+00 -1.38471877e+00  5.56555822e-01 -2.99437489e+00\n  -1.05373546e+00  1.16470849e+00  4.71539895e-01 -4.39669324e-02\n  -2.88948147e+00 -4.47308885e-01  2.97429214e-01  2.14273006e-01\n  -5.58749422e-01 -2.05552010e+00  1.68421464e-01  2.12510130e-01\n  -1.39277531e+00  3.25321873e-01  6.11171161e-01  3.92649583e-01\n  -2.40725342e+00  1.73769235e+00 -2.02941326e+00 -3.32425704e-01\n  -2.33770497e-01  1.18637278e-02  3.36493976e+00 -3.04386022e-01\n  -2.67299397e-01 -1.80373755e+00  1.21358046e+00 -1.42097123e+00\n   4.34001020e-01 -1.47782606e+00 -1.89085676e+00 -6.75936983e-01\n   1.12579164e-01  4.46817728e-01  5.50969224e-01 -3.34400244e-01\n   7.61072029e-01  1.42873596e+00  1.24138057e+00 -8.76798811e-01\n  -2.29649228e+00  9.70589635e-01  2.24936043e+00  1.81298022e-01\n   1.37774942e+00 -4.04876743e+00  6.58777385e-01 -1.71004248e+00\n   1.70289285e+00 -1.55435376e+00  1.02619918e+00 -5.38904307e-01\n  -8.40436326e-01 -9.65411124e-01 -5.73626630e-01 -6.51164391e-01\n   1.15297042e+00 -4.88491560e-01  7.91064448e-01  2.20900346e+00\n  -4.24991330e-01 -5.82627242e-01  6.75522007e-01 -9.75188038e-01\n   1.49294138e-01  1.49586626e+00  4.64413094e-01 -2.54017317e-01\n  -2.47105919e+00  3.36953373e-01  2.16967608e-01 -2.09191405e+00\n  -2.06584799e+00 -1.36886942e+00  8.67335888e-01  2.56678334e-02\n   8.51543465e-01  3.02396430e+00  2.60177099e-02  3.53841773e-02\n   3.35004103e-01  3.16917966e-01  3.38919808e+00 -5.90826793e-01\n  -1.86514024e+00 -2.79066666e+00 -1.36044102e+00 -2.63977681e+00\n   2.18843738e+00  6.37762380e-01  3.29864140e+00 -1.90052657e+00\n  -2.92672614e+00  6.79865675e-01 -2.95374842e+00  2.98816561e+00\n  -1.40075687e+00 -5.15960128e-01  6.77816045e-01 -4.43359054e+00]\n [-2.57517756e-01 -8.64660389e-01 -1.09405633e+00 -1.62461482e+00\n   4.75110855e-01  1.96287919e+00 -1.71437736e-01 -1.89252770e-01\n   3.28217292e-01  2.42777342e-01  1.66076892e+00  9.74747777e-01\n   1.50658502e+00 -1.57338640e-01  1.47164170e+00 -1.97522938e+00\n   5.95846019e-01 -2.32030035e+00  2.05813155e-01 -1.72992341e+00\n  -5.36051518e-01 -2.19998352e-01 -1.15368738e+00  1.36477012e+00\n  -6.89918348e-01 -1.11180618e+00 -4.33122356e-01  4.08708976e+00\n  -8.24784339e-01 -1.90509947e+00 -7.82499911e-01 -3.29195131e-01\n   5.33079335e-01 -1.27709730e+00  2.30758452e-01  1.18861617e+00\n  -3.12118925e+00  1.63405286e+00  3.15620200e-02 -4.55323213e-01\n   7.54002042e-01  2.95747491e+00  1.63885649e+00 -3.16710249e-01\n   9.76204360e-01  7.87345083e-03 -2.60784574e+00  1.43871916e+00\n  -1.77095070e-01  1.36751869e+00 -1.35465582e+00 -6.54841558e-01\n   3.66147327e-01  3.29708398e+00  1.87274243e+00  2.38020175e+00\n  -9.79345030e-01 -2.08969140e-01 -9.09875661e-01  3.53553076e-02\n  -4.44102870e-01  6.67673128e-01  6.27702156e-01 -2.98588145e-01\n   1.32695716e-01  6.90957809e-01  2.83086196e+00 -9.16006417e-01\n   1.71413398e+00  2.00627162e-01 -4.93070800e-02 -1.85908243e+00\n  -4.80720793e-01  6.35029749e-01 -3.16805158e-01  3.64306628e+00\n  -1.28221144e+00 -5.05703692e-01 -1.53814782e+00 -1.95968385e+00\n   1.82958643e+00 -1.01653776e+00  6.08223165e-02 -4.65919131e-01\n  -2.73273585e-01 -6.59023048e-01  3.08326000e+00 -2.16355720e+00\n  -8.40167223e-01  7.75659793e-01  6.99719446e-01  1.22624617e+00\n  -7.80399658e-01  1.22616163e+00 -1.86697228e-01  2.48650615e+00\n  -3.33557867e-01  3.53463968e+00 -3.85344033e+00 -4.42122783e-01\n   3.19504556e+00 -3.61246571e-01 -2.89384880e+00  1.80452650e+00\n  -3.09901933e-01 -2.82277867e+00  2.01018250e+00 -1.19482993e-01\n  -3.30635448e+00 -1.70337354e-01 -1.32579080e+00  1.13828898e-01\n   1.47676744e+00  1.74382239e+00 -3.11143622e+00 -3.69839833e+00\n   1.27569980e+00  1.94607328e+00 -9.56366618e-01 -3.17990132e+00\n   3.85056399e-01 -7.41561689e-02  1.30584018e+00  5.92081236e-01\n   2.28994573e-01  6.74737164e-01  1.59570397e+00  1.73510445e+00\n  -9.62063766e-01  2.80586211e-01  1.77408486e+00 -2.15213539e+00\n  -1.62620293e-01 -1.28772601e-02  7.38021738e-01 -2.00578461e+00\n   1.53829572e+00  1.18738539e+00  2.06695598e+00 -3.82344981e+00\n  -1.71591089e+00 -4.74917080e-01 -1.13981756e+00  1.14658278e-01\n  -8.34045238e-01 -5.94801733e-01 -6.95909025e-01  7.12118078e-01\n  -5.13359074e-01 -1.63954214e+00  2.63327101e-01  3.31443042e-01\n   2.43044147e+00 -2.66914765e-01  4.54359371e+00 -1.11937473e+00\n  -3.02802079e+00  1.83475491e+00 -3.49138029e-01  6.83302951e-01\n  -6.10606235e-01  1.24765872e+00 -6.93507243e-01  1.09169505e+00\n  -6.47154348e-01  1.59350680e-01 -1.51629246e+00 -1.88185498e+00\n   1.37153165e+00 -1.10311357e+00 -1.83930168e-01 -6.98665796e-01\n  -3.06941870e+00 -7.68927531e-01 -5.49332700e-01 -3.91959099e-01\n   1.36886660e+00 -9.83813458e-01 -1.88547471e-01 -1.71708921e+00\n   1.83640695e+00  4.78083111e-01 -8.47491857e-01  3.01793067e-01\n   1.55107993e-01 -2.98550257e-01 -9.72363261e-02 -9.46233956e-02\n   2.35468629e+00  2.84193111e+00  2.82752944e+00 -8.24494121e-01\n  -5.54382893e-02  1.30589434e+00 -3.50379414e-03  5.94000805e-01\n   7.70918504e-01 -2.39186468e+00 -1.66258221e+00 -2.06611282e-01\n  -1.63556973e-01 -1.72323667e+00 -1.89419425e+00 -1.07144046e-01\n   1.37929442e+00 -7.96610999e-01 -6.73326074e-01  1.23838336e+00\n   4.30287132e-01 -3.70121254e-01  2.78299771e-02  2.13232579e+00\n  -5.02445704e-01 -2.30737820e+00 -2.54367134e+00  4.81430977e-01\n   4.55576197e-01 -2.40754363e+00 -1.80325273e-01  2.79516286e+00\n  -2.41385362e-01 -6.24218126e-01  1.28296607e+00  1.54051465e+00\n  -1.15405101e-01  2.69190721e-01  3.88693149e-01 -2.36040779e+00\n  -8.36082114e-01  1.01351905e+00 -2.95830971e-01  9.13271559e-01\n  -5.56602062e-02  3.14853823e-01  1.93419183e+00  1.86075647e+00\n  -9.41552572e-01 -6.96083840e-01 -2.20979625e+00  1.58596232e+00\n  -3.76744924e-01 -1.17991090e+00 -1.29846310e+00 -1.95568976e+00\n   2.91559968e+00 -8.41603686e-01  1.58786487e+00 -1.64706578e+00\n  -1.72490150e+00  2.31370217e+00  2.27561758e+00  2.43636920e+00\n  -5.76789491e-01 -3.40745007e+00  2.08429378e+00 -4.91406365e-01\n  -4.60926108e+00 -8.44593929e-01  6.59660936e-01 -3.19349900e+00\n  -6.93455263e-01 -9.79002643e-01  8.98798435e-01  1.80859554e-01\n   9.44696192e-01 -2.48842766e-01 -1.62811472e+00 -2.83670236e-01\n   1.76605545e+00  1.29089712e+00  8.88518248e-01 -3.68051015e-01\n  -2.36635416e+00 -8.26587109e-01  2.72388722e+00 -2.63881325e+00\n  -3.70925160e+00 -1.16687499e+00 -1.22417379e+00 -8.60452375e-01\n   2.54551218e-01  7.16115950e-01 -1.55548030e+00 -1.68389024e+00\n   1.78992565e+00  1.93139738e+00 -8.31893562e-01 -3.12135145e+00\n   1.10774451e+00  8.07931413e-01  2.46610704e-01 -2.38167025e+00\n   1.43611832e+00 -6.52662263e-01 -9.07734685e-01 -4.59873790e-01\n   4.14301607e-01  1.84502686e+00 -1.40861515e+00  1.27162978e+00]]\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n[[-0.92027882 -0.24674402 -0.52863945 ...  0.25775476 -0.38344573\n   0.51808207]\n [-0.55416171  0.23490929  0.2762469  ... -0.24514078 -0.95390835\n   0.62899072]\n [ 0.46210166  0.49681644  1.01272096 ... -0.529298    0.32104184\n  -0.70423211]\n ...\n [-0.63996582 -0.39708151  0.36910408 ...  0.69895885  1.73736389\n  -0.10471945]\n [-0.243801    0.4780299  -0.29454778 ... -0.13877387 -0.47896282\n  -0.77195936]\n [-0.54595015 -0.7982178   0.17068149 ... -0.43297713 -0.77589837\n   0.45407349]]\n[[ 0.37224245  0.66034361 -0.65179631 ... -1.03542485  0.73280889\n  -0.23051297]\n [ 0.99168732  0.34877628  0.42116472 ... -0.3561015  -0.19427474\n  -0.20697834]\n [-0.608716   -0.2412576  -0.42209535 ...  0.51154935 -0.65380418\n  -0.45753079]\n ...\n [-0.78040147 -0.15689569  1.69383367 ... -0.28026127 -0.60237934\n   0.74057257]\n [ 0.5765623   1.02835415 -0.38533325 ... -0.50703243 -0.4646143\n   0.57071372]\n [ 0.28380121  0.34817277  0.31992719 ...  0.28382262  0.74908079\n  -0.74342133]]\n[[0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]]\n[ 1.  30.   0.2]\n[0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#=================================================================================\n",
    "# Setup\n",
    "#=================================================================================\n",
    "\n",
    "#update_policy   = func_update_policy(Tmax, use_x0)\n",
    "\n",
    "# let's break down func_update_policy here\n",
    "\"\"\"\n",
    "update policy network\n",
    ":param Tmax:\n",
    ":param use_x0:\n",
    ":param accumulators:\n",
    ":return:\n",
    "\"\"\"\n",
    "accumulators = None\n",
    "U = tensor.tensor3('U') # Inputs\n",
    "Q = tensor.tensor3('Q') # Noise\n",
    "\n",
    "if use_x0:\n",
    "    x0_ = tensor.matrix('x0_')\n",
    "else:\n",
    "    x0  = policy_net.params['x0']\n",
    "    x0_ = tensor.alloc(x0, U.shape[1], x0.shape[0])\n",
    "\n",
    "log_z_0  = policy_net.get_outputs_0(x0_, log=True)\n",
    "r, log_z = policy_net.get_outputs(U, Q, x0_, log=True)\n",
    "\n",
    "# Learning rate\n",
    "lr = tensor.scalar('lr')\n",
    "\n",
    "A = tensor.tensor3('A')\n",
    "R = tensor.matrix('R')\n",
    "b = tensor.matrix('b')\n",
    "M = tensor.matrix('M')\n",
    "\n",
    "logpi_0 = tensor.sum(log_z_0*A[0], axis=-1)*M[0]\n",
    "logpi_t = tensor.sum(log_z*A[1:],  axis=-1)*M[1:]\n",
    "\n",
    "print obj\n",
    "print trainables\n",
    "for x in trainables:\n",
    "    print x.get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Elemwise{add,no_inplace}.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enforce causality\n",
    "Mcausal = theanotools.zeros((Tmax-1, Tmax-1))\n",
    "for i in xrange(Mcausal.shape[0]):\n",
    "    Mcausal[i,i:] = 1\n",
    "Mcausal = theanotools.shared(Mcausal, 'Mcausal')\n",
    "\n",
    "J0 = logpi_0*R[0]\n",
    "J0 = tensor.mean(J0)\n",
    "J  = (logpi_t.T).dot(Mcausal).dot(R[1:]*M[1:])\n",
    "J  = tensor.nlinalg.trace(J)/J.shape[0]\n",
    "\n",
    "J += J0\n",
    "\n",
    "# Second term\n",
    "Jb0 = logpi_0*b[0]\n",
    "Jb0 = tensor.mean(Jb0)\n",
    "Jb  = logpi_t*b[1:]\n",
    "Jb  = tensor.mean(tensor.sum(Jb, axis=0))\n",
    "\n",
    "J -= Jb0 + Jb\n",
    "\n",
    "# Objective function\n",
    "obj = -J + policy_net.get_regs(x0_, r, M)# + 0.0005*entropy\n",
    "\n",
    "policy_net.get_regs(x0_, r, M)\n",
    "\n",
    "trainables = [policy_net.params[k]\n",
    "                for k in policy_net.params if k not in policy_net.config['fix']]\n",
    "\n",
    "trainables\n",
    "policy_net.type\n",
    "\n",
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "DisconnectedInputError",
     "evalue": " \nBacktrace when that variable is created:\n\n  File \"/home/hongli/anaconda2/envs/scientific/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/hongli/anaconda2/envs/scientific/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/hongli/anaconda2/envs/scientific/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2714, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/hongli/anaconda2/envs/scientific/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2818, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/hongli/anaconda2/envs/scientific/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2878, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-14-927f04a3102c>\", line 48, in <module>\n    seed=config['policy_seed'], name='policy')\n  File \"pyrl/gru.py\", line 287, in __init__\n    self.params[k] = theanotools.shared(v, k)\n  File \"pyrl/theanotools.py\", line 17, in shared\n    return theano.shared(np.asarray(x, theano.config.floatX), name=name)\n",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mDisconnectedInputError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-37c2ea4d9023>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# lr = lr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_sgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hongli/anaconda2/envs/scientific/lib/python2.7/site-packages/theano/gradient.pyc\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(cost, wrt, consider_constant, disconnected_inputs, add_names, known_grads, return_disconnected, null_gradients)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_to_app_to_idx\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0;32mand\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrad_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m             \u001b[0mhandle_disconnected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0mgrad_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisconnected_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hongli/anaconda2/envs/scientific/lib/python2.7/site-packages/theano/gradient.pyc\u001b[0m in \u001b[0;36mhandle_disconnected\u001b[0;34m(var)\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mdisconnected_inputs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raise'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m                 \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable_trace_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mDisconnectedInputError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m                 raise ValueError(\"Invalid value for keyword \"\n",
      "\u001b[0;31mDisconnectedInputError\u001b[0m:  \nBacktrace when that variable is created:\n\n  File \"/home/hongli/anaconda2/envs/scientific/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/hongli/anaconda2/envs/scientific/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/hongli/anaconda2/envs/scientific/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2714, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/hongli/anaconda2/envs/scientific/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2818, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/hongli/anaconda2/envs/scientific/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2878, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-14-927f04a3102c>\", line 48, in <module>\n    seed=config['policy_seed'], name='policy')\n  File \"pyrl/gru.py\", line 287, in __init__\n    self.params[k] = theanotools.shared(v, k)\n  File \"pyrl/theanotools.py\", line 17, in shared\n    return theano.shared(np.asarray(x, theano.config.floatX), name=name)\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# SGD\n",
    "policy_sgd = Adam(policy_net.trainables, accumulators=accumulators)\n",
    "if policy_net.type == 'simple':\n",
    "    i = policy_net.index('Wrec')\n",
    "    grads = tensor.grad(obj, policy_net.trainables)\n",
    "    grads[i] += policy_net.get_dOmega_dWrec(-J, r)\n",
    "    norm, grads, updates = policy_sgd.get_updates(obj, lr, grads=grads)\n",
    "\n",
    "else:\n",
    "    # norm, grads, updates = policy_sgd.get_updates(obj, lr)\n",
    "    # break get_dpdates here\n",
    "    loss = obj\n",
    "    grads = None\n",
    "    # lr = lr\n",
    "    if grads is None:\n",
    "        grads = tensor.grad(loss, policy_sgd.trainables)\n",
    "\n",
    "    # Clipping\n",
    "    norm  = tensor.sqrt(sum([tensor.sqr(g).sum() for g in grads]))\n",
    "    m     = theanotools.clipping_multiplier(norm, max_norm)\n",
    "    grads = [m*g for g in grads]\n",
    "\n",
    "    # Safeguard against numerical instability\n",
    "    new_cond = tensor.or_(tensor.or_(tensor.isnan(norm), tensor.isinf(norm)),\n",
    "                            tensor.or_(norm < 0, norm > 1e10))\n",
    "    grads = [tensor.switch(new_cond, np.float32(0), g) for g in grads]\n",
    "\n",
    "    # Safeguard against numerical instability\n",
    "    #cond  = tensor.or_(norm < 0, tensor.or_(tensor.isnan(norm), tensor.isinf(norm)))\n",
    "    #grads = [tensor.switch(cond, np.float32(0), g) for g in grads]\n",
    "\n",
    "    # New values\n",
    "    t       = policy_sgd.time + 1\n",
    "    lr_t    = lr*tensor.sqrt(1. - beta2**t)/(1. - beta1**t)\n",
    "    means_t = [beta1*m + (1. - beta1)*g for g, m in zip(grads, policy_sgd.means)]\n",
    "    vars_t  = [beta2*v + (1. - beta2)*tensor.sqr(g) for g, v in zip(grads, policy_sgd.vars)]\n",
    "    steps   = [lr_t*m_t/(tensor.sqrt(v_t) + epsilon)\n",
    "                for m_t, v_t in zip(means_t, vars_t)]\n",
    "\n",
    "    # Updates\n",
    "    updates  = [(x, x - step) for x, step in zip(policy_sgd.trainables, steps)]\n",
    "    updates += [(m, m_t) for m, m_t in zip(policy_sgd.means, means_t)]\n",
    "    updates += [(v, v_t) for v, v_t in zip(policy_sgd.vars, vars_t)]\n",
    "    updates += [(policy_sgd.time, t)]\n",
    "    \n",
    "if use_x0:\n",
    "    args = [x0_]\n",
    "else:\n",
    "    args = []\n",
    "args += [U, Q, A, R, b, M, lr]\n",
    "\n",
    "\n",
    "update_policy = theano.function(args, norm, updates=updates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_baseline = func_update_baseline(use_x0)\n",
    "\n",
    "# Start training from here\n",
    "iter_start = 0\n",
    "\n",
    "# Keep track of best results\n",
    "best_iter   = -1\n",
    "best_reward = -np.inf\n",
    "best_perf   = None\n",
    "best_params = policy_net.get_values()\n",
    "best_baseline_params = baseline_net.get_values()\n",
    "\n",
    "# Initial states\n",
    "init   = None\n",
    "init_b = None\n",
    "# Performance history\n",
    "perf             = None\n",
    "training_history = []\n",
    "trials_tot       = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=================================================================================\n",
    "# Train\n",
    "#=================================================================================\n",
    "\n",
    "        if hasattr(self.task, 'start_session'):\n",
    "            self.task.start_session(self.rng)\n",
    "\n",
    "        grad_norms_policy   = []\n",
    "        grad_norms_baseline = []\n",
    "\n",
    "        tstart = datetime.datetime.now()\n",
    "        try:\n",
    "            for iter_ in xrange(iter_start, max_iter+1):\n",
    "                if iter_ % checkfreq == 0 or iter_ == max_iter:\n",
    "                    if hasattr(self.task, 'n_validation'):\n",
    "                        n_validation = self.task.n_validation\n",
    "                    if n_validation > 0:\n",
    "                        #-----------------------------------------------------------------\n",
    "                        # Validation\n",
    "                        #-----------------------------------------------------------------\n",
    "\n",
    "                        # Report\n",
    "                        elapsed = utils.elapsed_time(tstart)\n",
    "                        print(\"After {} updates ({})\".format(iter_, elapsed))\n",
    "\n",
    "                        # RNG state\n",
    "                        rng_state = self.rng.get_state()\n",
    "\n",
    "                        # Trials\n",
    "\n",
    "                        #for bandit task, generate blockID randomly every batch\n",
    "\n",
    "\n",
    "                        n = 0\n",
    "                        iter = 0\n",
    "                        blockID = []\n",
    "                        while n < n_validation:\n",
    "                            blocklength = random.randint(20,50)\n",
    "                            for x in range(blocklength):\n",
    "                                blockID.append(iter%2)\n",
    "                            n += blocklength\n",
    "                            iter += 1\n",
    "\n",
    "\n",
    "                        trials = [self.task.get_condition(self.rng, self.dt, blockID[i])\n",
    "                                   for i in xrange(n_validation)]\n",
    "                        #trials = n_validation\n",
    "                        #\n",
    "                        (U, Q, Q_b, Z, Z_b, A, R, M, init_, init_b_, x0_, x0_b_,\n",
    "                         perf_) = self.run_trials(trials, progress_bar=True)\n",
    "                        if hasattr(self.task, 'update'):\n",
    "                            self.task.update(perf_)\n",
    "\n",
    "                        # Termination condition\n",
    "                        terminate = False\n",
    "                        if hasattr(self.task, 'terminate'):\n",
    "                            if self.task.terminate(perf_):\n",
    "                                terminate = True\n",
    "\n",
    "                        # Save\n",
    "                        mean_reward = np.sum(R*M)/n_validation\n",
    "                        record = {\n",
    "                            'iter':        iter_,\n",
    "                            'mean_reward': mean_reward,\n",
    "                            'n_trials':    trials_tot,\n",
    "                            'perf':        perf_\n",
    "                            }\n",
    "                        if mean_reward > best_reward or terminate:\n",
    "                            best_iter   = iter_\n",
    "                            best_reward = mean_reward\n",
    "                            best_perf   = perf_\n",
    "                            best_params          = self.policy_net.get_values()\n",
    "                            best_baseline_params = self.baseline_net.get_values()\n",
    "\n",
    "                            record['new_best'] = True\n",
    "                            training_history.append(record)\n",
    "                        else:\n",
    "                            record['new_best'] = False\n",
    "                            training_history.append(record)\n",
    "\n",
    "                        # Save\n",
    "                        save = {\n",
    "                            'iter':                    iter_,\n",
    "                            'config':                  self.config,\n",
    "                            'policy_config':           self.policy_net.config,\n",
    "                            'baseline_config':         self.baseline_net.config,\n",
    "                            'policy_masks':            self.policy_net.get_masks(),\n",
    "                            'baseline_masks':          self.baseline_net.get_masks(),\n",
    "                            'current_policy_params':   self.policy_net.get_values(),\n",
    "                            'current_baseline_params': self.baseline_net.get_values(),\n",
    "                            'best_iter':               best_iter,\n",
    "                            'best_reward':             best_reward,\n",
    "                            'best_perf':               best_perf,\n",
    "                            'best_policy_params':      best_params,\n",
    "                            'best_baseline_params':    best_baseline_params,\n",
    "                            'rng_state':               rng_state,\n",
    "                            'init':                    init,\n",
    "                            'init_b':                  init_b,\n",
    "                            'perf':                    perf,\n",
    "                            'training_history':        training_history,\n",
    "                            'trials_tot':              trials_tot,\n",
    "                            'net_sgd':                 self.policy_sgd.get_values(),\n",
    "                            'baseline_sgd':            self.baseline_sgd.get_values()\n",
    "                            }\n",
    "                        utils.save(savefile, save)\n",
    "\n",
    "                        # Reward\n",
    "                        items = OrderedDict()\n",
    "                        items['Best reward'] = '{} (iteration {})'.format(best_reward,\n",
    "                                                                          best_iter)\n",
    "                        items['Mean reward'] = '{}'.format(mean_reward)\n",
    "\n",
    "                        # Performance\n",
    "                        if perf_ is not None:\n",
    "                            items.update(perf_.display(output=False))\n",
    "\n",
    "                        # Value prediction error\n",
    "                        V = np.zeros_like(R)\n",
    "                        for k in xrange(V.shape[0]):\n",
    "                            V[k] = np.sum(R[k:]*M[k:], axis=0)\n",
    "                        error = np.sqrt(np.sum((Z_b - V)**2*M)/np.sum(M))\n",
    "                        items['Prediction error'] = '{}'.format(error)\n",
    "\n",
    "                        # Gradient norms\n",
    "                        if len(grad_norms_policy) > 0:\n",
    "                            if DEBUG:\n",
    "                                items['|grad| (policy)']   = [len(grad_norms_policy)] + [f(grad_norms_policy)\n",
    "                                                              for f in [np.min, np.median, np.max]]\n",
    "                                items['|grad| (baseline)'] = [len(grad_norms_baseline)] + [f(grad_norms_baseline)\n",
    "                                                              for f in [np.min, np.median, np.max]]\n",
    "                            grad_norms_policy   = []\n",
    "                            grad_norms_baseline = []\n",
    "\n",
    "                        # Print\n",
    "                        utils.print_dict(items)\n",
    "\n",
    "                        # Target reward reached\n",
    "                        if best_reward >= self.config['target_reward']:\n",
    "                            print(\"Target reward reached.\")\n",
    "                            return\n",
    "\n",
    "                        # Terminate\n",
    "                        if terminate:\n",
    "                            print(\"Termination criterion satisfied.\")\n",
    "                            return\n",
    "                    else:\n",
    "                        '''\n",
    "                        #-----------------------------------------------------------------\n",
    "                        # Ongoing learning\n",
    "                        #-----------------------------------------------------------------\n",
    "\n",
    "                        if not training_history:\n",
    "                            training_history.append(perf)\n",
    "                        if training_history[0] is None:\n",
    "                            training_history[0] = perf\n",
    "\n",
    "                        # Save\n",
    "                        save = {\n",
    "                            'iter':                    iter,\n",
    "                            'config':                  self.config,\n",
    "                            'policy_config':           self.policy_net.config,\n",
    "                            'baseline_config':         self.baseline_net.config,\n",
    "                            'masks_p':                 self.policy_net.get_masks(),\n",
    "                            'masks_b':                 self.baseline_net.get_masks(),\n",
    "                            'current_policy_params':   self.policy_net.get_values(),\n",
    "                            'current_baseline_params': self.baseline_net.get_values(),\n",
    "                            'rng_state':               self.rng.get_state(),\n",
    "                            'init':                    init,\n",
    "                            'init_b':                  init_b,\n",
    "                            'perf':                    perf,\n",
    "                            'training_history':        training_history,\n",
    "                            'trials_tot':              trials_tot,\n",
    "                            'net_sgd':                 self.policy_sgd.get_values(),\n",
    "                            'baseline_sgd':            self.baseline_sgd.get_values()\n",
    "                            }\n",
    "                        utils.save(savefile, save)\n",
    "                        '''\n",
    "                        if perf is not None:\n",
    "                            perf.display()\n",
    "\n",
    "                        # Termination condition\n",
    "                        terminate = False\n",
    "                        if hasattr(self.task, 'terminate'):\n",
    "                            if perf is not None and self.task.terminate(perf):\n",
    "                                terminate = True\n",
    "                        '''\n",
    "                        # Report\n",
    "                        if iter % 100 == 1:\n",
    "                            elapsed = utils.elapsed_time(tstart)\n",
    "                            print(\"After {} updates ({})\".format(iter, elapsed))\n",
    "                            if perf is not None:\n",
    "                                perf.display()\n",
    "                        '''\n",
    "                        # Terminate\n",
    "                        if terminate:\n",
    "                            print(\"Termination criterion satisfied.\")\n",
    "                            return\n",
    "\n",
    "                if iter_ == max_iter:\n",
    "                    print(\"Reached maximum number of iterations ({}).\".format(iter_))\n",
    "                    sys.exit(0)\n",
    "\n",
    "                #-------------------------------------------------------------------------\n",
    "                # Run trials\n",
    "                #-------------------------------------------------------------------------\n",
    "\n",
    "                # Trial conditions\n",
    "                if hasattr(self.task, 'n_gradient'):\n",
    "                    n_gradient = self.task.n_gradient\n",
    "\n",
    "                # for bandit task, generate blockID randomly every batch\n",
    "\n",
    "                n = 0\n",
    "                iter = 0\n",
    "                blockID_grad = []\n",
    "                while n < n_gradient:\n",
    "                    blocklength = random.randint(20, 50)\n",
    "                    for x in range(blocklength):\n",
    "                        blockID_grad.append(iter % 2)\n",
    "                    n += blocklength\n",
    "                    iter += 1\n",
    "\n",
    "                trials = [self.task.get_condition(self.rng, self.dt, blockID_grad[i])\n",
    "                          for i in xrange(n_gradient)]\n",
    "\n",
    "                # Run trials\n",
    "                (U, Q, Q_b, Z, Z_b, A, R, M, init, init_b, x0, x0_b,\n",
    "                 perf, r_policy, r_value) = self.run_trials(trials,\n",
    "                                                            init=init, init_b=init_b,\n",
    "                                                            return_states=True, perf=perf)\n",
    "\n",
    "                #-------------------------------------------------------------------------\n",
    "                # Update baseline\n",
    "                #-------------------------------------------------------------------------\n",
    "\n",
    "                baseline_inputs = np.concatenate((r_policy, A), axis=-1)\n",
    "\n",
    "                # Compute return\n",
    "                R_b = np.zeros_like(R)\n",
    "                for k in xrange(R.shape[0]):\n",
    "                    R_b[k] = np.sum(R[k:]*M[k:], axis=0)\n",
    "\n",
    "                if use_x0:\n",
    "                    args = [x0_b]\n",
    "                else:\n",
    "                    args = []\n",
    "                args += [baseline_inputs[:-1], Q_b, R_b, M, baseline_lr]\n",
    "                b, norm_b, rmse = update_baseline(*args)\n",
    "                #print(\"Prediction error = {}\".format(rmse))\n",
    "\n",
    "                norm_b = float(norm_b)\n",
    "                #print(\"norm_b = {}\".format(norm_b))\n",
    "                if np.isfinite(norm_b):\n",
    "                    grad_norms_baseline.append(float(norm_b))\n",
    "\n",
    "                #-------------------------------------------------------------------------\n",
    "                # Update policy\n",
    "                #-------------------------------------------------------------------------\n",
    "\n",
    "                if use_x0:\n",
    "                    args = [x0]\n",
    "                else:\n",
    "                    args = []\n",
    "                args += [U[:-1], Q, A, R, b, M, lr]\n",
    "                norm = update_policy(*args)\n",
    "\n",
    "                norm = float(norm)\n",
    "                #print(\"norm = {}\".format(norm))\n",
    "                if np.isfinite(norm):\n",
    "                    grad_norms_policy.append(norm)\n",
    "\n",
    "                trials_tot += n_gradient\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Training interrupted by user during iteration {}.\".format(iter_))\n",
    "            sys.exit(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyrl.gru.GRU"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
